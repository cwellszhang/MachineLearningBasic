{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "基础版\n",
    "==="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先，我们要知道信息增益的概念，信息增益是经验熵和经验条件熵之差，这里有引出了经验熵和经验条件熵的概念，要想知道这些概念我们还要首先知道什么是熵。\n",
    "\n",
    "在信息论中，熵是表示随机变量的不确定性的度量，随机变量X的熵定义为\n",
    "$$H(X)=-\\sum_{i=1}^np_ilogp_i$$\n",
    "熵只依赖于X的分布，而与X的取值无关，所以也可将X的熵记作H(p)\n",
    "$$H(p)=-\\sum_{i=1}^np_ilogp_i$$\n",
    "熵越大，随机变量的不确定性就越大。\n",
    "\n",
    "设随机变量（X，Y），其联合概率分布为\n",
    "$$P(X=x_i,Y=y_i)=p_{ij},i=1,2,..,n,j=1,2,...,m$$\n",
    "条件熵H(Y|X)表示在已知随机变量X的条件下随机变量Y的不确定性，随机变量X给定的条件下随机变量Y的条件熵H（Y|X），\n",
    "定义为X给定条件下Y的条件概率分布的熵对X的数学期望\n",
    "$$H(Y|X)=\\sum_{i=1}^np_iH(Y|X=x_i)$$\n",
    "这里， $p_i=P(X=x_i),i=1,2,...n$\n",
    "当熵和条件熵中的概率由数据估计(特别是极大似然估计)得到时，所对应的熵与条件熵分别称为经验熵和经验条件熵,此时,如果有0概率,令0log0=0.\n",
    "\n",
    "信息增益表示得知特征X的信息而使得类Y的信息的不确定性减少的程度。\n",
    "特征A对训练数据集D的信息增益g(D,A)定义为：集合D的经验熵H(D)与特征A给定条件下D的经验条件熵H(D|A)之差，即\n",
    "$$g(D,A)=H(D)-H(D|A)$$\n",
    "信息增益也称为互信息，决策树中学习的信息增益等价于训练数据集中类与特征的互信息。\n",
    "\n",
    "对公式的理解：经验熵H（D）表示对数据集D进行分类的不确定性，而条件经验熵H(D|A)表示在特征A给定的条件下对数据集D进行分类的不确定性。\n",
    "那么他们的差，即信息增益，就表示由于特征A而使得数据集D的分类的不确定性减少的程度。信息增益依赖于特征，不同的特征往往有不同的信息增益，\n",
    "信息增益大的特征具有更强的分类能力。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#定义简单鉴定的数据集函数  \n",
    "def createDataSet():  \n",
    "    dataSet = [[1,1,'yes'],  \n",
    "       [1,1,'yes'],  \n",
    "       [1,0,'no'],  \n",
    "       [0,1,'no'],  \n",
    "       [0,1,'no']]  \n",
    "    labels = ['no surfacing','flippers']  \n",
    "    return dataSet, labels  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mydat, labels =createDataSet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#计算给定数据集的香农熵,熵定义为信息的期望值，它代表随即变量的不确定性  \n",
    "from math import log  \n",
    "import operator  \n",
    "def calcShannonEnt(dataSet):  \n",
    "    numEntries = len(dataSet)  \n",
    "    labelCounts = {}  \n",
    "    for featVec in dataSet:  \n",
    "        currentLabel = featVec[-1]  \n",
    "        if currentLabel not in labelCounts.keys():  \n",
    "            labelCounts[currentLabel] = 0  \n",
    "        labelCounts[currentLabel] += 1 #这样也可以，书上没有错行    \n",
    "    shannonEnt = 0.0  \n",
    "    print(labelCounts)\n",
    "    for key in labelCounts:   \n",
    "        prob = float(labelCounts[key])/numEntries  \n",
    "        print(key,labelCounts[key],prob)   \n",
    "        shannonEnt -= prob * log(prob,2)#首先计算熵，它的作用是要用它计算每个特征的信息增益  \n",
    "    return shannonEnt  #熵越高混合的数据也越多  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个函数的作用：我们需要计算每个特征划分数据集的结果计算一次信息熵，然后判断按照哪个特征划分数据集是做好的划分方式。（也就是信息增益大的那个数据集）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面我们说了：信息增益是经验熵和经验条件熵之差，这个的划分数据集就是为了计算经验条件熵."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#得到熵之后，我们就可以按照获取最大信息增益的方法划分数据集，\n",
    "#那么如何划分数据集以及如何度量信息增益\n",
    "def splitDataSet(dataSet, axis, value):\n",
    "    retDataSet = []\n",
    "    for featVec in dataSet:\n",
    "        if featVec[axis] == value:\n",
    "            reducedFeatVec = featVec[:axis]\n",
    "            reducedFeatVec.extend(featVec[axis+1:])\n",
    "            retDataSet.append(reducedFeatVec)\n",
    "    #print(retDataSet)\n",
    "    return retDataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 'yes'], [1, 'yes'], [0, 'no']]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splitDataSet(mydat,0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#选取特征，划分数据集，计算得出最好的划分数据集的特征  \n",
    "def chooseBestFeatureToSplit(dataSet):  \n",
    "    numFeatures = len(dataSet[0]) - 1  #剩下的是特征的个数  \n",
    "    baseEntropy = calcShannonEnt(dataSet)#计算数据集的熵，放到baseEntropy中  \n",
    "    bestInfoGain = 0.0;bestFeature = -1  \n",
    "    for i in range(numFeatures):  \n",
    "        featList = [example[i] for example in dataSet]  \n",
    "        uniqueVals = set(featList)  \n",
    "        newEntropy = 0.0  \n",
    "        for value in uniqueVals:#下面是计算每种划分方式的信息熵,特征i个，每个特征value个值  \n",
    "            subDataSet = splitDataSet(dataSet, i ,value)  \n",
    "            prob = len(subDataSet)/float(len(dataSet))  \n",
    "            newEntropy += prob * calcShannonEnt(subDataSet)  \n",
    "        infoGain = baseEntropy - newEntropy  #计算i个特征的信息熵  \n",
    "        print(infoGain)  \n",
    "        if(infoGain > bestInfoGain):  \n",
    "            bestInfoGain = infoGain  \n",
    "            bestFeature = i  \n",
    "    return bestFeature  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'no': 3, 'yes': 2}\n",
      "no 3 0.6\n",
      "yes 2 0.4\n",
      "{'no': 2}\n",
      "no 2 1.0\n",
      "{'no': 1, 'yes': 2}\n",
      "no 1 0.3333333333333333\n",
      "yes 2 0.6666666666666666\n",
      "0.4199730940219749\n",
      "{'no': 1}\n",
      "no 1 1.0\n",
      "{'no': 2, 'yes': 2}\n",
      "no 2 0.5\n",
      "yes 2 0.5\n",
      "0.17095059445466854\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chooseBestFeatureToSplit(mydat)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#返回出现次数最多的分类名称  \n",
    "def majorityCnt(classList):  \n",
    "    classCount = {}  \n",
    "    for vote in classList:  \n",
    "        if vote not in classCount.keys():classCount[vote] = 0  \n",
    "        classCount[vote] += 1  \n",
    "    sortedClassCount  = sorted(classCount.iteritems(),key=operator.itemgetter(1), reverse=True)  \n",
    "    return sortedClassCount[0][0]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def createTree(dataSet,labels):  \n",
    "    classList = [example[-1] for example in dataSet]#将最后一行的数据放到classList中  \n",
    "    if classList.count(classList[0]) == len(classList):  \n",
    "        return classList[0]  \n",
    "    if len(dataSet[0]) == 1:#这里为什么是1呢？就是说特征数为1的时候  \n",
    "        return majorityCnt(classList)#就返回这个特征就行了，因为就这一个特征  \n",
    "    bestFeat = chooseBestFeatureToSplit(dataSet)  \n",
    "    print(bestFeat)  \n",
    "    bestFeatLabel = labels[bestFeat]#运行结果'no surfacing'  \n",
    "    myTree = {bestFeatLabel:{}}#运行结果{'no surfacing': {}}  \n",
    "    del(labels[bestFeat])  \n",
    "    featValues = [example[bestFeat] for example in dataSet]#第0个特征值  \n",
    "    uniqueVals = set(featValues)  \n",
    "    for value in uniqueVals:  \n",
    "        subLabels = labels[:]  \n",
    "        myTree[bestFeatLabel][value] = createTree(splitDataSet(dataSet,bestFeat,value),subLabels)  \n",
    "          \n",
    "    return myTree  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'no': 3, 'yes': 2}\n",
      "no 3 0.6\n",
      "yes 2 0.4\n",
      "{'no': 2}\n",
      "no 2 1.0\n",
      "{'no': 1, 'yes': 2}\n",
      "no 1 0.3333333333333333\n",
      "yes 2 0.6666666666666666\n",
      "0.4199730940219749\n",
      "{'no': 1}\n",
      "no 1 1.0\n",
      "{'no': 2, 'yes': 2}\n",
      "no 2 0.5\n",
      "yes 2 0.5\n",
      "0.17095059445466854\n",
      "0\n",
      "{'no': 1, 'yes': 2}\n",
      "no 1 0.3333333333333333\n",
      "yes 2 0.6666666666666666\n",
      "{'no': 1}\n",
      "no 1 1.0\n",
      "{'yes': 2}\n",
      "yes 2 1.0\n",
      "0.9182958340544896\n",
      "0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'no surfacing': {0: 'no', 1: {'flippers': {0: 'no', 1: 'yes'}}}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "createTree(mydat,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#下面代码我们将在真实的数据上使用决策树分类算法\n",
    "def classify(inputTree,featLabels,testVec):  \n",
    "    firstStr = list(inputTree.keys())[0]\n",
    "    secondDict = inputTree[firstStr]  \n",
    "    featIndex = featLabels.index(firstStr)#firstStr的索引  \n",
    "    for key in secondDict.keys():  \n",
    "        if testVec[featIndex] == key:  \n",
    "            if type(secondDict[key]).__name__ == 'dict':  \n",
    "                classLabel = classify(secondDict[key],featLabels,testVec)  \n",
    "            else:  \n",
    "                classLabel = secondDict[key]  \n",
    "    return classLabel   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'no': 3, 'yes': 2}\n",
      "no 3 0.6\n",
      "yes 2 0.4\n",
      "{'no': 2}\n",
      "no 2 1.0\n",
      "{'no': 1, 'yes': 2}\n",
      "no 1 0.3333333333333333\n",
      "yes 2 0.6666666666666666\n",
      "0.4199730940219749\n",
      "{'no': 1}\n",
      "no 1 1.0\n",
      "{'no': 2, 'yes': 2}\n",
      "no 2 0.5\n",
      "yes 2 0.5\n",
      "0.17095059445466854\n",
      "0\n",
      "{'no': 1, 'yes': 2}\n",
      "no 1 0.3333333333333333\n",
      "yes 2 0.6666666666666666\n",
      "{'no': 1}\n",
      "no 1 1.0\n",
      "{'yes': 2}\n",
      "yes 2 1.0\n",
      "0.9182958340544896\n",
      "0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'yes'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mydat,labels=createDataSet()\n",
    "myTree = createTree(mydat,labels)\n",
    "mydat,labels=createDataSet()\n",
    "classify(myTree,labels,[1,1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
