{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adaboost 算法\n",
    "======"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adaboost主要解决两个问题，一是在每一轮如何改变训练数据的权值或概率分布；二是如何将弱分类器组合成一个强分类器。\n",
    "1. 关于第一个问题，Adaboost的做法是提高那些被前一轮弱分类器误分类样本的权值，而降低那些被正确分类的权值\n",
    "2. 关于第二个问题, Adaboost采取加权多数表决的方法.具体地，加大分类误差率小的弱分类器的权值，使其在表决中起较大的作用。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "算法流程如下:\n",
    "\n",
    "输入:训练数据集 T = {$(x_1,y_1),(x_2,y_2),...(x_N,y_N)$}, y$\\in \\gamma={-1,+1}$\n",
    "\n",
    "输出: 最终分类器G(x)\n",
    "\n",
    "1. 初始化训练数据集的权值分布\n",
    "$$D_1=(\\omega_{11},...\\omega_{1i},...,\\omega_{1N}), \\omega_{1i}={1 \\over N},i=1,2,...N$$\n",
    "2. 对m=1,2,...M\n",
    "\n",
    " (2.1) 使用具有权值分布$D_m$的训练数据集学习，得到基本分类器\n",
    "$$G_m(x) = \\chi \\rightarrow \\{-1,+1\\}$$\n",
    " (2.2) 计算$G_m(x)$在训练数据集上的分类误差率\n",
    "$$e_m=P(G_m(x_i)\\neq y_i) = \\sum_{i=1}^N\\omega_{mi}I(G_m(x_i)\\neq y_i)$$\n",
    " (2.3) 计算$G_m(x)的系数$\n",
    "$$\\alpha_m = {1\\over2}log{1-e_m \\over e_m}$$\n",
    " $a_m$随着$e_m$的减小而增大，所以分类误差率越小的基本分类器在最终的分类器中作用越大\n",
    " \n",
    " (2.4) 更新训练数据集的权值分布\n",
    "$$D_{m+1} = (\\omega_{m+1,1},...,\\omega_{m+1,i},...\\omega_{m+1,N})$$\n",
    "$$\\omega_{m+1,i}={\\omega_{mi}\\over Z_m}exp(-\\alpha_my_iG_m(x_i)) ,i=1,2,...,N $$\n",
    "这里,$Z_m$是规范化因子\n",
    "$$Z_m = \\sum_{i=1}^Nexp(-\\alpha_my_iG_m(x_i))$$\n",
    "它使$D_{m+1}称为一个概率分布$\n",
    "\n",
    "3. 构建基本分类器的线性组合\n",
    "$$f(x)=\\sum_{m=1}^M\\alpha_mG_m(x)$$\n",
    "得到最终分类器\n",
    "$$G(x)=sign(f(x)) = sign(\\sum_{m=1}^M\\alpha_mG_m(x))$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadSimpData():  \n",
    "    datMat=matrix([[1.,2.1],  \n",
    "                   [2.,1.1],  \n",
    "                   [1.3,1.],  \n",
    "                   [1.,1.],  \n",
    "                   [2.,1.]])  \n",
    "    classLabels=[1.0,1.0,-1.0,-1.0,1.0]  \n",
    "    return datMat,classLabels  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stumpClassify(dataMatrix,dimen,threshVal,threshIneq):\n",
    "    \"\"\"\n",
    "    用于测试是否有某个值小于或大于我们的阈值\n",
    "    \"\"\"\n",
    "    retArray = ones((dataMatrix.shape[0],1))\n",
    "    if threshIneq == 'lt':\n",
    "        retArray[dataMatrix[:,dimen] <= threshVal] = -1.0\n",
    "    else:\n",
    "        retArray[dataMatrix[:,dimen] > threshVal] = -1.0\n",
    "    return retArray\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import *\n",
    "def buildStump(dataArr,classLabels,D):\n",
    "    \"\"\"\n",
    "    将minError设置为无穷大\n",
    "    对数据集中的每一个属性:\n",
    "        对每个步长（第二层循环）:\n",
    "            对每个不等号:\n",
    "                建立一棵单层决策树并利用加权数据集对其进行测试\n",
    "                如果错误率低于minError，则将当前的决策树设为最佳单层决策树\n",
    "    返回最佳单层决策树\n",
    "    \"\"\"\n",
    "    dataMatrix = mat(dataArr)\n",
    "    labelMat =mat(classLabels).T\n",
    "    m,n = dataMatrix.shape\n",
    "    numSteps = 10.0;bestStump={};bestClasEst = mat(zeros((m,1)))\n",
    "    #下面按照上述伪代码实现\n",
    "    minError  = inf\n",
    "    for i in range(n):\n",
    "        rangeMin = dataMatrix[:,i].min()\n",
    "        rangeMax = dataMatrix[:,i].max()\n",
    "        stepSize = (rangeMax-rangeMin)/numSteps\n",
    "        for j in range(-1,int(numSteps)+1):\n",
    "            for inequal in ['lt','gt']:\n",
    "                threshVal = (rangeMin+float(j)*stepSize )\n",
    "                #阈值一边的会被分类到-1，另一边的会被分类到+1.\n",
    "                predictVals = stumpClassify(dataMatrix,i,threshVal,inequal)\n",
    "                errArr = mat(ones((m,1)))\n",
    "                errArr[predictVals==labelMat]=0\n",
    "                weightedError  = D.T*errArr\n",
    "#                 print(\"split: dim %d, thresh %.2f, thresh inequal: %s, the weighted error: %.3f\" \\\n",
    "#                       %(i, threshVal, inequal, weightedError)  )\n",
    "                if weightedError < minError:\n",
    "                    minError = weightedError\n",
    "                    bestClasEst = predictVals.copy()\n",
    "                    bestStump['dim']=i\n",
    "                    bestStump['thresh'] = threshVal\n",
    "                    bestStump['ineq'] = inequal\n",
    "    return bestStump,minError,bestClasEst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'dim': 0, 'ineq': 'lt', 'thresh': 1.3}, matrix([[ 0.2]]), array([[-1.],\n",
       "        [ 1.],\n",
       "        [-1.],\n",
       "        [-1.],\n",
       "        [ 1.]]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from numpy import *\n",
    "D= mat(ones((5,1))/5)\n",
    "datamat,classlabels = loadSimpData()\n",
    "buildStump(datamat,classlabels,D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def adaBoostTrainDS(dataArr,classLabels,numIt=40):\n",
    "    \"\"\"\n",
    "    对每次迭代：\n",
    "        利用buildStump找到最佳的单层决策树\n",
    "        将最佳单层决策树加入数组\n",
    "        计算分类器系数alpha\n",
    "        计算新的权重D\n",
    "        更新累计类别估计值\n",
    "        如果错误率为0.0，跳出循环\n",
    "    \"\"\"\n",
    "    weakClassArr = []\n",
    "    m = shape(dataArr)[0]\n",
    "    D = mat(ones((m,1))/m)\n",
    "    aggClassEst  = mat(zeros((m,1)))\n",
    "    for i in range(numIt):\n",
    "        bestStump,error,classEst= buildStump(dataArr,classLabels,D)\n",
    "        print(\"D:\",D.T)\n",
    "        alpha = float(0.5*log((1.0-error)/max(error,1e-16)))\n",
    "        bestStump['alpha']=alpha\n",
    "        weakClassArr.append(bestStump)\n",
    "        print(\"classEst:\",classEst.T)\n",
    "        expon = multiply(-1*alpha*mat(classlabels).T,classEst)\n",
    "        D = multiply(D,exp(expon))\n",
    "        D = D/D.sum()\n",
    "        aggClassEst += alpha*classEst\n",
    "        print(\"aggClassEst:\",aggClassEst.T)\n",
    "        aggErrors = multiply(sign(aggClassEst)!=mat(classlabels).T,ones((m,1)))\n",
    "        errorRate = aggErrors.sum()/m\n",
    "        print(\"total error:\",errorRate,\"\\n\")\n",
    "        if errorRate==0.0:break\n",
    "    return weakClassArr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D: [[ 0.2  0.2  0.2  0.2  0.2]]\n",
      "classEst: [[-1.  1. -1. -1.  1.]]\n",
      "aggClassEst: [[-0.69314718  0.69314718 -0.69314718 -0.69314718  0.69314718]]\n",
      "total error: 0.2 \n",
      "\n",
      "D: [[ 0.5    0.125  0.125  0.125  0.125]]\n",
      "classEst: [[ 1.  1. -1. -1. -1.]]\n",
      "aggClassEst: [[ 0.27980789  1.66610226 -1.66610226 -1.66610226 -0.27980789]]\n",
      "total error: 0.2 \n",
      "\n",
      "D: [[ 0.28571429  0.07142857  0.07142857  0.07142857  0.5       ]]\n",
      "classEst: [[ 1.  1.  1.  1.  1.]]\n",
      "aggClassEst: [[ 1.17568763  2.56198199 -0.77022252 -0.77022252  0.61607184]]\n",
      "total error: 0.0 \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'alpha': 0.6931471805599453, 'dim': 0, 'ineq': 'lt', 'thresh': 1.3},\n",
       " {'alpha': 0.9729550745276565, 'dim': 1, 'ineq': 'lt', 'thresh': 1.0},\n",
       " {'alpha': 0.8958797346140273,\n",
       "  'dim': 0,\n",
       "  'ineq': 'lt',\n",
       "  'thresh': 0.90000000000000002}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adaBoostTrainDS(datamat,classlabels,9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def adaClassify(datToClass,classifierArr):\n",
    "    dataMatrix = mat(datToClass)\n",
    "    m = dataMatrix.shape[0]\n",
    "    aggClassEst = mat(zeros((m,1)))\n",
    "    for i in range(len(calssifierArr)):\n",
    "        classEst = stumpClassify(dataMatrix,classifierArr[i]['dim'],\\\n",
    "                                classifierArr[i]['thresh'],\\\n",
    "                                classifierArr[i]['ineq'])\n",
    "        aggClassEst += classifierArr[i]['alpha']*classEst\n",
    "        print(aggClassEst)\n",
    "    return sign(aggClassEst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D: [[ 0.2  0.2  0.2  0.2  0.2]]\n",
      "classEst: [[-1.  1. -1. -1.  1.]]\n",
      "aggClassEst: [[-0.69314718  0.69314718 -0.69314718 -0.69314718  0.69314718]]\n",
      "total error: 0.2 \n",
      "\n",
      "D: [[ 0.5    0.125  0.125  0.125  0.125]]\n",
      "classEst: [[ 1.  1. -1. -1. -1.]]\n",
      "aggClassEst: [[ 0.27980789  1.66610226 -1.66610226 -1.66610226 -0.27980789]]\n",
      "total error: 0.2 \n",
      "\n",
      "D: [[ 0.28571429  0.07142857  0.07142857  0.07142857  0.5       ]]\n",
      "classEst: [[ 1.  1.  1.  1.  1.]]\n",
      "aggClassEst: [[ 1.17568763  2.56198199 -0.77022252 -0.77022252  0.61607184]]\n",
      "total error: 0.0 \n",
      "\n",
      "[[-0.69314718]\n",
      " [ 0.69314718]]\n",
      "[[-1.66610226]\n",
      " [ 1.66610226]]\n",
      "[[-2.56198199]\n",
      " [ 2.56198199]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "matrix([[-1.],\n",
       "        [ 1.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calssifierArr=adaBoostTrainDS(datamat,classlabels,10)\n",
    "adaClassify([[0,0],[5,5]],calssifierArr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
